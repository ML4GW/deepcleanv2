{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from clean.data import DeepCleanInferenceDataset\n",
    "from clean.infer import OnlineInference\n",
    "from clean.model import InferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chiajui.chou/deepcleanv2/projects/clean'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['device', 'sample_rate', 'hoft_dir', 'witness_dir', 'outdir', 'train_dir'])\n",
      "dict_keys(['fname', 'channels', 'kernel_length', 'freq_low', 'freq_high', 'batch_size', 'train_duration', 'test_duration', 'valid_frac', 'train_stride', 'inference_sampling_rate', 'start_offset', 'filt_order'])\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# Loading configuration from YAML\n",
    "clean_config_file = \"/home/chiajui.chou/deepcleanv2/projects/clean/config_clean.yaml\"\n",
    "with open(clean_config_file, 'r') as file:\n",
    "    clean_config = yaml.safe_load(file)\n",
    "print(clean_config.keys())\n",
    "\n",
    "train_config_file = os.path.join(clean_config['train_dir'], \"config.yaml\")\n",
    "with open(train_config_file, 'r') as file:\n",
    "    train_config = yaml.safe_load(file)\n",
    "# print(train_config.keys())\n",
    "print(train_config['data'].keys())\n",
    "print(train_config['data']['inference_sampling_rate'])\n",
    "\n",
    "# train_hparams_file = os.path.join(clean_config['train_dir'], \"hparams.yaml\")\n",
    "# with open(train_hparams_file, 'r') as file:\n",
    "#     train_hparams = yaml.safe_load(file)\n",
    "# print(train_hparams.keys())\n",
    "\n",
    "# Initialize InferenceModel\n",
    "model = InferenceModel(\n",
    "    clean_config['train_dir'],\n",
    "    clean_config['sample_rate'],\n",
    "    clean_config['device'],\n",
    ")\n",
    "\n",
    "# Initialize DeepCleanInferenceDataset\n",
    "inference_dataset = DeepCleanInferenceDataset(\n",
    "    hoft_dir=clean_config['hoft_dir'],\n",
    "    witness_dir=clean_config['witness_dir'],\n",
    "    model=model,\n",
    "    device=clean_config['device'],\n",
    ")\n",
    "\n",
    "# Initialize OnlineInference\n",
    "online_inference = OnlineInference(\n",
    "    dataset=inference_dataset,\n",
    "    model=model,\n",
    "    outdir=clean_config['outdir'],\n",
    "    device=clean_config['device']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online_inference.model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the online inference process for a number of iterations (e.g., 100)\n",
    "# for k in range(300):\n",
    "#     online_inference.predict_and_write()\n",
    "#     online_inference.dataset.update()\n",
    "#     print(f\"iteration {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InferenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250916847\n",
      "4096\n",
      "X_inference:\n",
      "4096\n",
      "2048\n",
      "y_inference:\n",
      "4096\n",
      "2048\n",
      "torch.Size([5, 21, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Initialize DeepCleanInferenceDataset\n",
    "inference_dataset = DeepCleanInferenceDataset(\n",
    "    hoft_dir=clean_config['hoft_dir'],\n",
    "    witness_dir=clean_config['witness_dir'],\n",
    "    model=model,\n",
    "    device=clean_config['device'],\n",
    ")\n",
    "\n",
    "print(inference_dataset.t0)\n",
    "print(inference_dataset.kernel_size)\n",
    "print(\"X_inference:\")\n",
    "print(inference_dataset.X_inference.kernel_size)\n",
    "print(inference_dataset.X_inference.stride)\n",
    "print(\"y_inference:\")\n",
    "print(inference_dataset.y_inference.kernel_size)\n",
    "print(inference_dataset.y_inference.stride)\n",
    "\n",
    "# print(inference_dataset.X_inference.X.shape)\n",
    "# print(inference_dataset.y_inference.X.shape)\n",
    "for i in inference_dataset.X_inference:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250916847\n",
      "torch.Size([5, 21, 4096])\n",
      "tensor([0.6945, 0.7658, 0.7648,  ..., 0.3913, 0.3912, 0.3911], device='cuda:0')\n",
      "torch.Size([5, 4096])\n",
      "tensor([-1.8676e-19, -2.1235e-19, -1.8091e-19,  ...,  1.2468e-19,\n",
      "         1.4771e-19,  1.4411e-19], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inference_dataset.t0)\n",
    "X_dset = inference_dataset.X_inference\n",
    "y_dset = inference_dataset.y_inference\n",
    "for X, y in zip(X_dset, y_dset):\n",
    "    print(X.shape)\n",
    "    print(X[0,0,:])\n",
    "    print(y.shape)\n",
    "    print(y[0,:])\n",
    "\n",
    "# inference_dataset.update()\n",
    "# print(inference_dataset.t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.jit._script.RecursiveScriptModule'>\n"
     ]
    }
   ],
   "source": [
    "device = clean_config['device']\n",
    "# print(clean_config)\n",
    "train_dir = clean_config['train_dir']\n",
    "t_model = torch.jit.load(os.path.join(train_dir, \"model.pt\")).to(device)\n",
    "print(type(t_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([5, 21, 4096])\n",
      "torch.Size([5, 4096])\n"
     ]
    }
   ],
   "source": [
    "witness = next(iter(X_dset))\n",
    "print(witness.device)\n",
    "print(witness.shape)\n",
    "\n",
    "pred = t_model(witness)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_pad': 0.2, 'filter_pad': 0.8}\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(train_config['model']['metric']['init_args'])\n",
    "inrerence_sampling_rate = train_config['data']['inference_sampling_rate']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean-IeYuvgpU-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
